{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccf55b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the text into embeddings\n",
    "train_test = \"The quick brown fox jumps over the lazy dog and chased swift playfully\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efea59ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'and', 'chased', 'swift', 'playfully']\n",
      "{'The': 0, 'quick': 1, 'brown': 2, 'fox': 3, 'jumps': 4, 'over': 5, 'the': 6, 'lazy': 7, 'dog': 8, 'and': 9, 'chased': 10, 'swift': 11, 'playfully': 12}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "tokenize = re.split(r'([,./\"]|--|\\s)',train_test)\n",
    "train_tokenize = [item for item in tokenize if item is not None and item.strip()]\n",
    "vocab = {item:val for val,item in enumerate(train_tokenize)}\n",
    "print(train_tokenize)\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73f410f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 2, 8, 12, 10, 6, 11, 3]\n",
      "the brown dog playfully chased the swift fox\n"
     ]
    }
   ],
   "source": [
    "# we can add more regex for this if we want \n",
    "class TokenizersV1:\n",
    "    def __init__(self,vocab):\n",
    "        self.word_to_int = vocab\n",
    "        self.int_to_word = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self,word):\n",
    "        word = word.lower()\n",
    "        preprocess = re.split(r'([,.?]|--|\\s)',word)\n",
    "        preprocess = [item for item in preprocess if item is not None and item.strip()]\n",
    "        itr = [self.word_to_int[i] for i in preprocess]\n",
    "        return itr\n",
    "    \n",
    "    def decode(self,ids):\n",
    "        text = \" \".join([self.int_to_word[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([.,?/])','/1',text)\n",
    "        return text\n",
    "\n",
    "tokenize = TokenizersV1(vocab)\n",
    "sample = \"The Brown dog playfully chased the swift fox\"\n",
    "encoded = tokenize.encode(sample)\n",
    "decoded = tokenize.decode(encoded)\n",
    "print(encoded)\n",
    "print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a9b6ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'quick': 1, 'brown': 2, 'fox': 3, 'jumps': 4, 'over': 5, 'the': 6, 'lazy': 7, 'dog': 8, 'and': 9, 'chased': 10, 'swift': 11, 'playfully': 12, '<|endoftext|>': 13, '<|unk|>': 14}\n"
     ]
    }
   ],
   "source": [
    "# tokenizer that handles unknown words\n",
    "train_data = \"The quick brown fox jumps over the lazy dog and chased swift playfully\"\n",
    "word = re.split(r\"([.,?]|--|\\s)\",train_data)\n",
    "word = [item for item in word if item is not None and item.strip()]\n",
    "word.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "word = {item:idx for idx,item in enumerate(word)} \n",
    "vocab = word\n",
    "print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95497516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 2, 8, 12, 10, 6, 11, 3, 14, 14]\n",
      "the brown dog playfully chased the swift fox <|unk|> <|unk|>\n",
      "  \n",
      "[14, 14, 14, 14, 14, 6, 14]\n",
      "<|unk|> <|unk|> <|unk|> <|unk|> <|unk|> the <|unk|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class TokenizersV2:\n",
    "    def __init__(self,vocab):\n",
    "        self.word_to_int = vocab\n",
    "        self.int_to_word = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        text = text.lower()\n",
    "        words = re.split(r'([,.?_/\\[\\]]|--|\\s)', text)\n",
    "        words = [item for item in words if item is not None and item.strip()]\n",
    "        words = [item if item in self.word_to_int else \"<|unk|>\" for item in words]\n",
    "        ids = [self.word_to_int[word] for word in words]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self,ids):\n",
    "        decode  = \" \".join([self.int_to_word[i] for i in ids])\n",
    "        decode = re.sub(r'\\s + ([,.;:?\\ ])',r'/1',decode)\n",
    "        return decode\n",
    "    \n",
    "\n",
    "tokenize = TokenizersV2(vocab)\n",
    "sample1 = \"The Brown dog playfully chased the swift fox virat bmw\"\n",
    "encoded1 = tokenize.encode(sample1)\n",
    "decoded1 = tokenize.decode(encoded1)\n",
    "\n",
    "sample2 = \"Hello, do you like the tea\"\n",
    "encoded2 = tokenize.encode(sample2)\n",
    "decoded2 = tokenize.decode(encoded2)\n",
    "\n",
    "print(encoded1)\n",
    "print(decoded1)\n",
    "print(\"  \")\n",
    "\n",
    "print(encoded2)\n",
    "print(decoded2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb8d6b0",
   "metadata": {},
   "source": [
    "byte pair encoding -> which break words into subword units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c44d131",
   "metadata": {},
   "source": [
    "BPE breaks down words that aren't in its predefined vocob into smaller subword units or even individual characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8c5deb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version:  0.9.0\n"
     ]
    }
   ],
   "source": [
    "# byte pair encoding\n",
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version: \",version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "536400a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 2954, 2197, 5372]\n"
     ]
    }
   ],
   "source": [
    "tokenize = tiktoken.get_encoding(\"gpt2\")\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknowplace\"  \n",
    "integers = tokenize.encode(text,allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "643634ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknowplace\n"
     ]
    }
   ],
   "source": [
    "strings = tokenize.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ad8a387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x,[15496, 11, 466, 345]\n",
      "y: [11, 466, 345, 588]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "x = integers[:context_size]\n",
    "y = integers[1:context_size+1]\n",
    "print(f\"x,{x}\")\n",
    "print(f\"y: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04233858",
   "metadata": {},
   "source": [
    "Data sampling using sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f52c773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496] ----> 11\n",
      "[15496, 11] ----> 466\n",
      "[15496, 11, 466] ----> 345\n",
      "[15496, 11, 466, 345] ----> 588\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,context_size+1):\n",
    "    context = integers[:i]\n",
    "    desired = integers[i]\n",
    "    print(context,\"---->\",desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acaff200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello ---> ,\n",
      "Hello, --->  do\n",
      "Hello, do --->  you\n",
      "Hello, do you --->  like\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,context_size+1):\n",
    "    context = integers[:i]\n",
    "    desired = integers[i]\n",
    "    print(tokenize.decode(context), \"--->\", tokenize.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c12dbbd",
   "metadata": {},
   "source": [
    "implementing efficient data sampling with a sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac090b8f",
   "metadata": {},
   "source": [
    "its common to train LLM with input sizes of at least 256 to understand the meaning of stride = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15a89537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token length :  37\n",
      "33\n",
      "(tensor([ 262, 2612,  286,  262]), tensor([2612,  286,  262, 1748]))\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset ,DataLoader\n",
    "import tensorflow as tensor\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self,txt,tokenize,max_length,stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        token_ids = tokenize.encode(txt)\n",
    "        print(\"token length : \",len(token_ids))\n",
    "        \n",
    "        for i in range(0,len(token_ids)-max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1 : i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index],self.target_ids[index]\n",
    "    \n",
    "x = \"In the heart of the city stood the old library, a relic from a bygone era. its stone walls bore the marks of time,and ivy clung tightly to its facade\"\n",
    "\n",
    "tokenize = tiktoken.get_encoding(\"gpt2\")\n",
    "max_length = 4\n",
    "stride = 1\n",
    "train = GPTDatasetV1(x,tokenize,max_length,stride)\n",
    "\n",
    "\n",
    "print(train.__len__())\n",
    "print(train.__getitem__(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8375e049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token length :  37\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "def create_dataloader_v1(txt,batch_size=1,max_length=4,stride=1,shuffle=False,drop_last=True):\n",
    "    tokenize = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt,tokenize,max_length,stride)\n",
    "    dataloader = DataLoader(dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=shuffle,\n",
    "                    drop_last=drop_last\n",
    "                )\n",
    "    return dataloader\n",
    "\n",
    "dataloader = create_dataloader_v1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a605f72",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 818,  262, 2612,  286]]), tensor([[ 262, 2612,  286,  262]])]\n",
      "[tensor([[ 262, 2612,  286,  262]]), tensor([[2612,  286,  262, 1748]])]\n",
      "[tensor([[2612,  286,  262, 1748]]), tensor([[ 286,  262, 1748, 6204]])]\n",
      "[tensor([[ 286,  262, 1748, 6204]]), tensor([[ 262, 1748, 6204,  262]])]\n",
      "[tensor([[ 262, 1748, 6204,  262]]), tensor([[1748, 6204,  262, 1468]])]\n",
      "[tensor([[1748, 6204,  262, 1468]]), tensor([[6204,  262, 1468, 5888]])]\n",
      "[tensor([[6204,  262, 1468, 5888]]), tensor([[ 262, 1468, 5888,   11]])]\n",
      "[tensor([[ 262, 1468, 5888,   11]]), tensor([[1468, 5888,   11,  257]])]\n",
      "[tensor([[1468, 5888,   11,  257]]), tensor([[ 5888,    11,   257, 26341]])]\n",
      "[tensor([[ 5888,    11,   257, 26341]]), tensor([[   11,   257, 26341,   422]])]\n",
      "[tensor([[   11,   257, 26341,   422]]), tensor([[  257, 26341,   422,   257]])]\n",
      "[tensor([[  257, 26341,   422,   257]]), tensor([[26341,   422,   257,   416]])]\n",
      "[tensor([[26341,   422,   257,   416]]), tensor([[  422,   257,   416, 21260]])]\n",
      "[tensor([[  422,   257,   416, 21260]]), tensor([[  257,   416, 21260,  6980]])]\n",
      "[tensor([[  257,   416, 21260,  6980]]), tensor([[  416, 21260,  6980,    13]])]\n",
      "[tensor([[  416, 21260,  6980,    13]]), tensor([[21260,  6980,    13,   663]])]\n",
      "[tensor([[21260,  6980,    13,   663]]), tensor([[6980,   13,  663, 7815]])]\n",
      "[tensor([[6980,   13,  663, 7815]]), tensor([[  13,  663, 7815, 7714]])]\n",
      "[tensor([[  13,  663, 7815, 7714]]), tensor([[  663,  7815,  7714, 18631]])]\n",
      "[tensor([[  663,  7815,  7714, 18631]]), tensor([[ 7815,  7714, 18631,   262]])]\n",
      "[tensor([[ 7815,  7714, 18631,   262]]), tensor([[ 7714, 18631,   262,  8849]])]\n",
      "[tensor([[ 7714, 18631,   262,  8849]]), tensor([[18631,   262,  8849,   286]])]\n",
      "[tensor([[18631,   262,  8849,   286]]), tensor([[ 262, 8849,  286,  640]])]\n",
      "[tensor([[ 262, 8849,  286,  640]]), tensor([[8849,  286,  640,   11]])]\n",
      "[tensor([[8849,  286,  640,   11]]), tensor([[286, 640,  11, 392]])]\n",
      "[tensor([[286, 640,  11, 392]]), tensor([[  640,    11,   392, 21628]])]\n",
      "[tensor([[  640,    11,   392, 21628]]), tensor([[   11,   392, 21628,    88]])]\n",
      "[tensor([[   11,   392, 21628,    88]]), tensor([[  392, 21628,    88,   537]])]\n",
      "[tensor([[  392, 21628,    88,   537]]), tensor([[21628,    88,   537,  2150]])]\n",
      "[tensor([[21628,    88,   537,  2150]]), tensor([[   88,   537,  2150, 17707]])]\n",
      "[tensor([[   88,   537,  2150, 17707]]), tensor([[  537,  2150, 17707,   284]])]\n",
      "[tensor([[  537,  2150, 17707,   284]]), tensor([[ 2150, 17707,   284,   663]])]\n",
      "[tensor([[ 2150, 17707,   284,   663]]), tensor([[17707,   284,   663, 43562]])]\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(dataloader)\n",
    "for batch in data_iter:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee96e795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_id len :  37\n",
      "vocab_size :  50257\n",
      "third row :  tensor([[-0.2196, -0.3792,  0.7671]], grad_fn=<EmbeddingBackward0>)\n",
      "input_ids :  torch.Size([37, 3])\n"
     ]
    }
   ],
   "source": [
    "# converting token ID into embedding\n",
    "x = \"In the heart of the city stood the old library, a relic from a bygone era. its stone walls bore the marks of time,and ivy clung tightly to its facade\"\n",
    "tokenize = tiktoken.get_encoding(\"gpt2\")\n",
    "token_id = tokenize.encode(x)\n",
    "input_id = torch.tensor(token_id)\n",
    "print(\"input_id len : \",len(input_id))\n",
    "vocab_size = tokenize.n_vocab\n",
    "print(\"vocab_size : \", vocab_size)\n",
    "ouput_dim = 3\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size,ouput_dim)\n",
    "print(\"third row : \" ,embedding_layer(torch.tensor([2])))\n",
    "print(\"input_ids : \" , embedding_layer(input_id).shape)     #multiple with row of there token_id of 37 rows of embedding layers\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
